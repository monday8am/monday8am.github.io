---
layout: post
title:  "Exporting a Lottie Animation to Video Using Media3"
date:   2025-08-21 15:12:00 +0100
categories: blog
---

### Summary 

Exporting a Lottie animation to an MP4 file — *with audio* — is a powerful feature for any creative or media-rich Android app. Whether you’re building a story editor, motion graphics tool, or onboarding animation, the ability to render Lottie animations as sharable videos can significantly enhance your user experience. Besides, it's a common case inside very succesful applications like [Unfold](https://unfold.com/), [InStories](https://instories.com/) or [Storybeat](https://www.storybeat.com/) that provides thousands of templates with animations to the users and the tools for costumizing them adding photos, text or videos.

This post walks you through a clean and performant pipeline that renders Lottie to video using the [Media3 Transformer API](https://developer.android.com/media/transformer/overview) and OpenGL — and adds audio support along the way.


### What We'll Build

We'll implement a function:

```kotlin
suspend fun recordLottieToVideo(
  context: Context,
  lottieFrameFactory: LottieFrameFactory,
  audioUri: String,
  outputFilePath: String,
  onProgress: (Float) -> Unit = {},
  onSuccess: (fileSize: Long) -> Unit = {},
  onError: (Throwable) -> Unit = {},
  durationMs: Long,
  frameRate: Int = 30,
  width: Int = 1080,
  height: Int = 1920
)
```

This function:

* Renders Lottie frames to GPU textures
* Encodes them to video using Media3
* Adds an audio track via raw PCM decoding
* Outputs a complete `.mp4` file


### The Problem: Bridging Two Worlds

After some research, the solution to exporting Lottie animations with audio using Media3 comes down to connecting two seemingly unrelated pieces:

* On one side, we have `LottieDrawable`, a **regular Android Drawable** generated by Lottie that can draw animations onto any Canvas, including hardware-accelerated ones.

* On the other side, we have Media3's `Transformer`, which can be configured with a `RawAssetLoader` to receive custom video frames (as OpenGL texture IDs) and raw audio chunks.

But how do you go from a `LottieDrawable` — something that draws on a Canvas — to an OpenGL texture ID that Media3 can encode?

The missing link is **`ImageReader`**.

`ImageReader` gives us a Surface that:

* Can be drawn to using lockHardwareCanvas() (GPU-accelerated)

* Provides Image frames that can be uploaded to the GPU as textures

This makes it the perfect bridge between the CPU-based Lottie drawing world and the GPU-based Media3 encoding pipeline.

LottieDrawable --draws to--> ImageReader.surface (via lockHardwareCanvas)
ImageReader --outputs--> Image (GPU-compatible)
Image --uploaded to--> GL Texture
Texture --queued to--> RawAssetLoader --encoded by--> Media3 Transformer


### Step 1: Control Everything with `RawAssetLoader`

The `RawAssetLoader` gives you full manual control over how video and audio data is fed into the Media3 encoder:

```kotlin
.setAssetLoaderFactory(
  RawAssetLoaderFactory(
    audioFormat = audioFormat,
    videoFormat = videoFormat,
    onRawAssetLoaderCreated = { loader -> rawAssetLoaderDeferred.complete(loader) }
  )
)
```

From here on, you're responsible for queuing:

* OpenGL texture IDs with timestamps
* Raw PCM audio chunks with timestamps

The audio chunks can be read from a local audio file in the traditional, using a `MediaExtractor`, a `MediaCodec` and creating a pipeline of small bufferes that are extracted, decoded and then passd to the `RawAssetLoader`'s callback.

### Step 2: Render Lottie to a Hardware Canvas

Each frame of the Lottie animation is generated as a `Drawable` using a LottieDrawable. The most performant way to get that drawable into a video frame is to draw it into a hardware-accelerated canvas — not into a regular software Bitmap.

```kotlin
val hCanvas = imageReader.surface.lockHardwareCanvas()
try {
  lottieFrame.setBounds(0, 0, videoWidth, videoHeight)
  lottieFrame.draw(hCanvas)
} finally {
  imageReader.surface.unlockCanvasAndPost(hCanvas)
}
```

This approach is not just efficient — it’s the key to real-time performance. By using lockHardwareCanvas() from an ImageReader.Surface, you automatically get a hardware-backed Canvas that is connected to GPU memory.

#### ✅ Does Lottie use the GPU here?

Yes! Internally, `LottieDrawable.draw(Canvas)` chooses between a software (`renderAndDrawAsBitmap`) and hardware (`drawDirectlyToCanvas`) rendering path.

If the `Canvas` is hardware-accelerated — as it is when created from `lockHardwareCanvas()` — Lottie will automatically use GPU rendering via `drawDirectlyToCanvas()`.


### Step 3: Upload Frame as Texture via `ImageReader`

The unsung hero here is `ImageReader`. It provides the surface we draw to, and then lets us acquire the drawn image for use with OpenGL:

```kotlin
val imageReader = ImageReader.newInstance(width, height, PixelFormat.RGBA_8888, maxImages)
```

Once a frame is drawn, we acquire the image, upload it to an OpenGL texture and and enqueue the texture id returned into the `RawAssetLoader`:

```kotlin
val image = awaitForLastImage.await()
val textureId = uploadImageToGLTexture(image)

rawAssetLoader.queueInputTexture(textureId, presentationTimeUs)
```

This texture now represents a video frame and will be encoded by Media3 Transformer.

---

### Step 4: Decode and Enqueue Audio

In parallel, we decode audio from a URI into raw PCM chunks using a custom `AudioDecoder`. Each chunk is queued manually into the asset loader:

```kotlin
val (audioChunk, presentationTimeUs) = awaitForAudioChunk.await()

rawAssetLoader.queueAudioData(audioChunk, presentationTimeUs, isLast)
```

Synchronization between video frames and audio chunks is manual — but simple, since you’re in full control.

---

### Bringing It All Together

```kotlin
launch {
  repeat(totalFrames) { frameIndex ->
    // 1. Draw Lottie frame into hardware canvas
    // 2. Acquire image from ImageReader
    // 3. Upload image to GL texture
    // 4. Enqueue texture into Media3
  }
  rawAssetLoader.signalEndOfVideoInput()
}

launch {
  while (!endOfStream) {
    // 1. Decode next PCM chunk
    // 2. Enqueue it to Media3
  }
  rawAssetLoader.signalEndOfAudioInput()
}
```

I have used the coroutine `CompletableDeferred` calls as semaphores to suspend the thread and wait for the corresponding results in a typesafe way: the _assetLoaderDeferred_, _a_waitReadyForInput_ and _awaitForLastImage_ controls the flow and makes the whole operation a suspend function. This design also makes it easy to wrap the process inside a use case that emits a Flow of results.

The full solution can be found in the following repository:
[]


### Why This Approach Works

* `RawAssetLoader` gives full audio+video control
* `ImageReader` is optimized for canvas-to-texture use
* `lockHardwareCanvas()` ensures GPU memory drawing
* `LottieDrawable` already handles GPU rendering internally when used on a hardware canvas


### Future Improvements

* Add progress reporting (`onProgress: (Float) -> Unit`)
* Use a `SurfaceTexture` + GL rendering instead of `Canvas` for full GPU control
* Investigate using [Skottie (Skia GPU Lottie renderer)](https://skia.org/docs/user/modules/skottie/) for even faster native rendering


### References

- [Media3 Transformer API](https://developer.android.com/media/transformer/overview)
- [Airbnb Lottie Android](https://airbnb.io/lottie/#/)
- [ImageReader API](https://developer.android.com/reference/android/media/ImageReader)
- [`lockHardwareCanvas()`](https://developer.android.com/reference/android/view/Surface#lockHardwareCanvas%28%29)
- [SurfaceAssetLoader](https://developer.android.com/reference/androidx/media3/transformer/SurfaceAssetLoader)
- [ImageReader](https://developer.android.com/reference/android/media/ImageReader)
- [Image](https://developer.android.com/reference/android/media/Image)
- [CompletableDeferred](https://kotlinlang.org/api/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines/-completable-deferred/)
